name: "llama3_vision"
platform: "tensorrtllm"
max_batch_size: 8
dynamic_batching {
    preferred_batch_size: [4, 8]
    max_queue_delay_microseconds: 100
    default_queue_policy {
        timeout_action: DELAY
        default_timeout_microseconds: 1000000
    }
}
instance_group [
    {
        count: 2
        kind: KIND_GPU
        gpus: [ 0 ]
    }
]
