# Triton Server for LLaMA 3.2 Vision

This project provides a Dockerized NVIDIA Triton Inference Server for the LLaMA 3.2 11B Vision model, optimized with TensorRT-LLM. It supports two configurations: BF16 precision (no quantization) and INT8 quantization.

## Prerequisites

- **Docker**: Installed and running.
- **NVIDIA GPU**: With CUDA support and NVIDIA Docker runtime.
- **Hugging Face Token**: Required for model download.
- **Model Storage**: Directory at `/mnt/model_data` for caching models.

## Features

- Base image: `nvcr.io/nvidia/tritonserver:25.01-trtllm-python-py3`.
- Multi-GPU support (`CUDA_VISIBLE_DEVICES=0,1`).
- Two entrypoint options:
  - `entrypoint_no_quantization.sh`: BF16 precision for LLM engine.
  - `entrypoint_quantization.sh`: INT8 quantization for LLM engine.
- Dynamic entrypoint selection via environment variable or interactive prompt.

## Building the Image

```bash
docker build -t llama_vision_triton .
```

## Running the Container

Mount your model data and notebooks, and provide your Hugging Face token:

```bash
docker run --gpus all \
  -v /mnt/model_data:/model_data \
  -v $(pwd):/notebooks \
  -e HF_TOKEN=YOUR_HF_TOKEN \
  -p 8000:8000 -p 8001:8001 -p 8002:8002 -p 8888:8888 \
  llama_vision_triton:latest
```

### Entrypoint Selection

- **Interactive**: Add `-it` to prompt for BF16 or INT8:
  ```bash
  docker run -it --gpus all [other args] llama_vision_triton:latest
  ```
- **Environment Variable**: Specify `ENTRYPOINT_MODE`:
  - BF16: `-e ENTRYPOINT_MODE="no_quantization"`
  - INT8: `-e ENTRYPOINT_MODE="quantization"`
- **Default**: Runs BF16 if no mode is specified.

## Testing the Server

Send a request to generate a haiku based on an image:

```bash
curl -X POST localhost:8000/v2/models/ensemble/generate_stream \
  -d '{"id": "42", "text_input": "<|image|>If I had to write a haiku for this one", "image_url_input": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/assets/merlion.png", "parameters": {"max_tokens": 16, "beam_width": 1, "end_id": 128001, "pad_id": 128004, "top_k": 1, "top_p": 0, "stream": false, "temperature": 0}}'
```

Expected output: A haiku generated by the model based on the Merlion image.

## Environment Variables

- `HF_TOKEN`: Your Hugging Face token (required).
- `ENTRYPOINT_MODE`: `no_quantization` or `quantization` to select the entrypoint.

## Project Structure

- `Dockerfile`: Defines the container setup.
- `entrypoint.sh`: Wrapper script for entrypoint selection.
- `entrypoint_no_quantization.sh`: BF16 setup script.
- `entrypoint_quantization.sh`: INT8 setup script.

## Ports

- `8000`: HTTP endpoint.
- `8001`: gRPC endpoint.
- `8002`: Metrics endpoint.
- `8888`: Notebook access (if applicable).

## Notes

- Ensure `/mnt/model_data` exists and is writable for model caching.
- The container downloads the LLaMA 3.2 11B Vision model if not cached.
- Adjust `-v $(pwd):/notebooks` if you don’t need notebook support.

## Troubleshooting

- **Model Download Fails**: Check `HF_TOKEN` validity and network access.
- **GPU Errors**: Verify `--gpus all` and NVIDIA runtime configuration.
- **Port Conflicts**: Ensure ports 8000-8002 and 8888 are free.

---

This README reflects your exact commands and adds clarity for users. Let me know if you’d like to tweak anything further!